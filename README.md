# Fine-Tuning BERT with Wikidata
This repository contains the dataset constructed from Wikidata, used for finetuning BERT Pre-trained Language models, namely DistilBERT, RoBERTa, and BERT-Large models. Further, we tested the performance of the fine-tuned models over a human-annotated dataset.
